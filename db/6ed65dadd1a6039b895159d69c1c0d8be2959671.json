{"parentSha1":"5444b17082a3413816dcfce1721f581c2c2f07f2","path":"answers-db/books/Neural Networks and Deep Learning/2. How the backpropagation algorithm works/The four fundamental equations behind backpropagation/Alternate presentation of the equations of backpropagation","item":{"title":"Alternate presentation of the equations of backpropagation","attr":{"q":"<p>I've stated the equations of backpropagation (notably (BP1) and (BP2)) using the Hadamard product. This presentation may be disconcerting if you're unused to the Hadamard product. There's an alternative approach, based on conventional matrix multiplication, which some readers may find enlightening.</p>"},"sha1":"6ed65dadd1a6039b895159d69c1c0d8be2959671"},"kids":[{"title":"3","attr":{"q":"<p>By combining observations (1) and (2) show that</p>\n<p><span class=\"math display\">\\delta^l = \\Sigma&#39;(z^l) (w^{l+1})^T \\ldots \\Sigma&#39;(z^{L-1}) (w^L)^T \n    \\Sigma&#39;(z^L) \\nabla_a C\n  \\tag{35}</span></p>","a":"<p>We've already proved that Hadamard productinng <span class=\"math inline\">\\sigma &#39; (z_l)</span> is equal to multiplying <span class=\"math inline\">\\Sigma &#39; (z_l)</span>, and <span class=\"math inline\">\\Sigma &#39; (z_l)</span> can move it's multiplicative position. Rearranging the result of (2) we get</p>\n<p><span class=\"math display\">\\delta_l = W^\\intercal \\delta_{l+1} \\odot \\sigma &#39; (z_l) = W^\\intercal \\Sigma &#39; (z_l) \\delta_{l+1}</span></p>\n<p>which is recursive expression with the initial value of <span class=\"math inline\">\\delta^L = \\Sigma&#39;(z^L) \\nabla_a C</span>. We get the expression from the question when we serialize this recursive expression.</p>"},"sha1":"f94cad13c7bb4a71fbee036997d4bcb5e36d381e"},{"title":"1","attr":{"q":"<p>Show that (BP1) may be rewritten as</p>\n<p><span class=\"math display\">\\delta^L = \\Sigma&#39;(z^L) \\nabla_a C\n\\tag{33}</span></p>\n<p>where <span class=\"math inline\">\\Sigma&#39;(z^L)</span> is a square matrix whose diagonal entries are the values <span class=\"math inline\">\\sigma&#39;(z^L_j)</span>, and whose off-diagonal entries are zero. Note that this matrix acts on <span class=\"math inline\">\\nabla_a C</span> by conventional matrix multiplication.</p>","a":"<p>We can either prove it by using properties of diagonal matrices or using the chain rule of gradient. Former one is rather mundane so I'll go with latter one.</p>\n<p><span class=\"math inline\">\\delta_l</span> is gradient of <span class=\"math inline\">C</span> in terms of <span class=\"math inline\">z_l</span>, namely</p>\n<p><span class=\"math display\">\\delta^l = \\nabla_{z_l} C</span></p>\n<p>Since <span class=\"math inline\">a_l = \\sigma(z_l)</span>, we can apply chain rule with Jacobian matrix.</p>\n<p><span class=\"math display\">\\nabla_{z_l} C = {J_\\sigma (z_l)}^\\intercal \\nabla_{a_l} C</span></p>\n<p>When the function is element-wise operation the Jacobian matrix becomes a diagonal matrix. <span class=\"math inline\">\\sigma</span> is elementwise function therefore the Jacobian matrix becomes exactly the matrix that was given in the question.</p>\n<p><span class=\"math display\">= {\\Sigma&#39; (z_l)}^\\intercal \\nabla_{a_l} C</span></p>\n<p>Transpose doesn't change diagonal matrices so</p>\n<p><span class=\"math display\">= {\\Sigma&#39; (z_l)} \\nabla_{a_l} C</span></p>"},"sha1":"6f70934e588d04e883d816fe5be976f96aba3cf9"},{"title":"2","attr":{"q":"<p>Show that (BP2) may be rewritten as</p>\n<p><span class=\"math display\">\\delta^l = \\Sigma&#39;(z^l) (w^{l+1})^T \\delta^{l+1} \\tag{34}</span></p>","a":"<p>Just like the previous question, we can either prove it by using properties of diagonal matrices or applying one more chian rule to the answer of previous question. I'll go with latter one again.</p>\n<p>Let <span class=\"math inline\">T</span> be the function of multiplying and adding weights and biases.</p>\n<p><span class=\"math display\">T(a) = Wa + B</span></p>\n<p>Then we can define <span class=\"math inline\">z_{l+1}</span> in terms of <span class=\"math inline\">z_l</span>.</p>\n<p><span class=\"math display\">z_{l+1} = T \\circ \\sigma (z_l)</span></p>\n<p>Jacobian of <span class=\"math inline\">T \\circ \\sigma</span> is (chain rule)</p>\n<p><span class=\"math display\">J_{T \\sigma} = J_T (a_l) J_\\sigma (z_l)</span></p>\n<p>where <span class=\"math inline\">J_\\sigma (z_l)</span> is <span class=\"math inline\">\\Sigma &#39; (z_l)</span> (proven in the previous problem) and <span class=\"math inline\">J_T (a_l) = W</span> (because of linearity). Therefore we can simplify like this</p>\n<p><span class=\"math display\">= W \\Sigma &#39; (z_l)</span></p>\n<p>We are already given the gradient of <span class=\"math inline\">C</span> in terms of <span class=\"math inline\">z_{l+1}</span> which is <span class=\"math inline\">\\delta_{l+1}</span>, therefore</p>\n<p><span class=\"math display\">\\begin{aligned}\n    \\delta_l &amp;= \\left( W \\Sigma &#39; (z_l) \\right)^\\intercal \\delta_{l+1} \\\\\n             &amp;= W^\\intercal \\Sigma &#39; (z_l)^\\intercal \\delta_{l+1} \\\\\n\\end{aligned}</span></p>\n<p>Since <span class=\"math inline\">\\Sigma &#39; (z_l)</span> is diagonal we can remove the transpose and move it to the end and turn into Hadamard product.</p>\n<p><span class=\"math display\">= W^\\intercal \\delta_{l+1} \\odot \\sigma &#39; (z_l)</span></p>"},"sha1":"600d0c672155d4021896a9f2ac1e685dc5ad0b5f"}]}
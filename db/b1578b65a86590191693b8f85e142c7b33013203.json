{"parentSha1":"f2063b90a4aef8602640129e2df8a69f2cb98fc8","path":"answers-db/books/Neural Networks and Deep Learning/1. Using neural nets to recognize handwritten digits/Learning with gradient descent/3","item":{"title":"3","attr":{"q":"<p>An extreme version of gradient descent is to use a mini-batch size of just 1. That is, given a training input, <span class=\"math inline\">x</span>, we update our weights and biases according to the rules <span class=\"math inline\">w_k \\rightarrow w_k&#39; = w_k - \\eta \\partial C_x / \\partial w_k</span> and <span class=\"math inline\">b_l \\rightarrow b_l&#39; = b_l - \\eta \\partial C_x / \\partial b_l</span>. Then we choose another training input, and update the weights and biases again. And so on, repeatedly. This procedure is known as online, on-line, or incremental learning. In online learning, a neural network learns from just one training input at a time (just as human beings do). Name one advantage and one disadvantage of online learning, compared to stochastic gradient descent with a mini-batch size of, say, 20.</p>","a":"<h3 id=\"advantages\">Advantages</h3>\n<p>Computation speed.</p>\n<h3 id=\"disadvantages\">Disadvantages</h3>\n<p>Poorly estimatied <span class=\"math inline\">C</span>.</p>"},"sha1":"b1578b65a86590191693b8f85e142c7b33013203"},"kids":[]}
{
    "item": {
        "attr": {},
        "numAnswer": 3,
        "sha1": "f2063b90a4aef8602640129e2df8a69f2cb98fc8",
        "title": "Learning with gradient descent"
    },
    "kids": [
        {
            "attr": {
                "a": {
                    "content": "<p>By Cauchy-Schwarz inequality, <span class=\"math inline\">\\nabla C\n\\cdot \\Delta v</span>'s least possible value is <span\nclass=\"math inline\">- \\lVert \\nabla C \\rVert \\lVert \\Delta v\n\\rVert</span> and it only happens when <span class=\"math inline\">\\Delta\nv</span> is colinear to <span class=\"math inline\">\\nabla C</span>\ntherefore</p>\n<p><span class=\"math display\">\\text{min}(\\nabla C \\cdot \\Delta v) \\\\\n= - \\lVert \\nabla C \\rVert \\lVert \\eta \\nabla C \\rVert</span></p>\n<p><span class=\"math inline\">\\lVert \\eta \\nabla C \\rVert</span> should\nbe <span class=\"math inline\">\\epsilon</span> therefore</p>\n<p><span class=\"math display\">\\eta = {\\epsilon \\over \\lVert \\nabla C\n\\rVert}</span></p>",
                    "posixTime": 1591599357
                },
                "q": {
                    "content": "<p>Prove the assertion of the last paragraph. Hint: If you're not\nalready familiar with the Cauchy-Schwarz inequality, you may find it\nhelpful to familiarize yourself with it.</p>",
                    "posixTime": 1591599357
                }
            },
            "numAnswer": 1,
            "sha1": "00425e3210304345e443430fadd193926ebc3aa7",
            "title": "1"
        },
        {
            "attr": {
                "a": {
                    "content": "<p><img\nsrc=\"/answers/books/Neural%20Networks%20and%20Deep%20Learning/1.%20Using%20neural%20nets%20to%20recognize%20handwritten%20digits/Learning%20with%20gradient%20descent/2/IMG_0636.jpeg\" /></p>",
                    "posixTime": 1591599357
                },
                "q": {
                    "content": "<p>I explained gradient descent when <span class=\"math inline\">C</span>\nis a function of two variables, and when it's a function of more than\ntwo variables. What happens when <span class=\"math inline\">C</span> is a\nfunction of just one variable? Can you provide a geometric\ninterpretation of what gradient descent is doing in the one-dimensional\ncase?</p>",
                    "posixTime": 1591599357
                }
            },
            "numAnswer": 1,
            "sha1": "3bde7e77d851b7e18006524660894b07e00ae47e",
            "title": "2"
        },
        {
            "attr": {
                "a": {
                    "content": "<h3 id=\"advantages\">Advantages</h3>\n<p>Computation speed.</p>\n<h3 id=\"disadvantages\">Disadvantages</h3>\n<p>Poorly estimatied <span class=\"math inline\">C</span>.</p>",
                    "posixTime": 1591717109
                },
                "q": {
                    "content": "<p>An extreme version of gradient descent is to use a mini-batch size of\njust 1. That is, given a training input, <span\nclass=\"math inline\">x</span>, we update our weights and biases according\nto the rules <span class=\"math inline\">w_k \\rightarrow w_k&#39; = w_k -\n\\eta \\partial C_x / \\partial w_k</span> and <span\nclass=\"math inline\">b_l \\rightarrow b_l&#39; = b_l - \\eta \\partial C_x /\n\\partial b_l</span>. Then we choose another training input, and update\nthe weights and biases again. And so on, repeatedly. This procedure is\nknown as online, on-line, or incremental learning. In online learning, a\nneural network learns from just one training input at a time (just as\nhuman beings do). Name one advantage and one disadvantage of online\nlearning, compared to stochastic gradient descent with a mini-batch size\nof, say, 20.</p>",
                    "posixTime": 1591717109
                }
            },
            "numAnswer": 1,
            "sha1": "b1578b65a86590191693b8f85e142c7b33013203",
            "title": "3"
        }
    ],
    "parentSha1": "c73690766f702ac0bcd28dd2f0f00e04c1072586",
    "path": "answers-db/books/Neural Networks and Deep Learning/1. Using neural nets to recognize handwritten digits/Learning with gradient descent"
}
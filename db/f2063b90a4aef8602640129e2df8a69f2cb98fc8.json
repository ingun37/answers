{"parentSha1":"c73690766f702ac0bcd28dd2f0f00e04c1072586","path":"answers-db/books/Neural Networks and Deep Learning/1. Using neural nets to recognize handwritten digits/Learning with gradient descent","item":{"title":"Learning with gradient descent","attr":{},"sha1":"f2063b90a4aef8602640129e2df8a69f2cb98fc8"},"kids":[{"title":"3","attr":{"q":"<p>An extreme version of gradient descent is to use a mini-batch size of just 1. That is, given a training input, <span class=\"math inline\">x</span>, we update our weights and biases according to the rules <span class=\"math inline\">w_k \\rightarrow w_k&#39; = w_k - \\eta \\partial C_x / \\partial w_k</span> and <span class=\"math inline\">b_l \\rightarrow b_l&#39; = b_l - \\eta \\partial C_x / \\partial b_l</span>. Then we choose another training input, and update the weights and biases again. And so on, repeatedly. This procedure is known as online, on-line, or incremental learning. In online learning, a neural network learns from just one training input at a time (just as human beings do). Name one advantage and one disadvantage of online learning, compared to stochastic gradient descent with a mini-batch size of, say, 20.</p>","a":"<h3 id=\"advantages\">Advantages</h3>\n<p>Computation speed.</p>\n<h3 id=\"disadvantages\">Disadvantages</h3>\n<p>Poorly estimatied <span class=\"math inline\">C</span>.</p>"},"sha1":"b1578b65a86590191693b8f85e142c7b33013203"},{"title":"2","attr":{"q":"<p>I explained gradient descent when <span class=\"math inline\">C</span> is a function of two variables, and when it's a function of more than two variables. What happens when <span class=\"math inline\">C</span> is a function of just one variable? Can you provide a geometric interpretation of what gradient descent is doing in the one-dimensional case?</p>","a":"<p><img src=\"/answers/imgs/IMG_0636.jpeg\" /></p>"},"sha1":"3bde7e77d851b7e18006524660894b07e00ae47e"},{"title":"1","attr":{"q":"<p>Prove the assertion of the last paragraph. Hint: If you're not already familiar with the Cauchy-Schwarz inequality, you may find it helpful to familiarize yourself with it.</p>","a":"<p>By Cauchy-Schwarz inequality, <span class=\"math inline\">\\nabla C \\cdot \\Delta v</span>'s least possible value is <span class=\"math inline\">- \\lVert \\nabla C \\rVert \\lVert \\Delta v \\rVert</span> and it only happens when <span class=\"math inline\">\\Delta v</span> is colinear to <span class=\"math inline\">\\nabla C</span> therefore</p>\n<p><span class=\"math display\">\\text{min}(\\nabla C \\cdot \\Delta v) \\\\\n= - \\lVert \\nabla C \\rVert \\lVert \\eta \\nabla C \\rVert</span></p>\n<p><span class=\"math inline\">\\lVert \\eta \\nabla C \\rVert</span> should be <span class=\"math inline\">\\epsilon</span> therefore</p>\n<p><span class=\"math display\">\\eta = {\\epsilon \\over \\lVert \\nabla C \\rVert}</span></p>"},"sha1":"00425e3210304345e443430fadd193926ebc3aa7"}]}
{"parentSha1":"6ed65dadd1a6039b895159d69c1c0d8be2959671","path":"answers-db/books/Neural Networks and Deep Learning/2. How the backpropagation algorithm works/The four fundamental equations behind backpropagation/Alternate presentation of the equations of backpropagation/1","item":{"title":"1","attr":{"q":"<p>Show that (BP1) may be rewritten as</p>\n<p><span class=\"math display\">\\delta^L = \\Sigma&#39;(z^L) \\nabla_a C\n\\tag{33}</span></p>\n<p>where <span class=\"math inline\">\\Sigma&#39;(z^L)</span> is a square matrix whose diagonal entries are the values <span class=\"math inline\">\\sigma&#39;(z^L_j)</span>, and whose off-diagonal entries are zero. Note that this matrix acts on <span class=\"math inline\">\\nabla_a C</span> by conventional matrix multiplication.</p>","a":"<p>We can either prove it by using properties of diagonal matrices or using the chain rule of gradient. Former one is rather mundane so I'll go with latter one.</p>\n<p><span class=\"math inline\">\\delta_l</span> is gradient of <span class=\"math inline\">C</span> in terms of <span class=\"math inline\">z_l</span>, namely</p>\n<p><span class=\"math display\">\\delta^l = \\nabla_{z_l} C</span></p>\n<p>Since <span class=\"math inline\">a_l = \\sigma(z_l)</span>, we can apply chain rule with Jacobian matrix.</p>\n<p><span class=\"math display\">\\nabla_{z_l} C = {J_\\sigma (z_l)}^\\intercal \\nabla_{a_l} C</span></p>\n<p>When the function is element-wise operation the Jacobian matrix becomes a diagonal matrix. <span class=\"math inline\">\\sigma</span> is elementwise function therefore the Jacobian matrix becomes exactly the matrix that was given in the question.</p>\n<p><span class=\"math display\">= {\\Sigma&#39; (z_l)}^\\intercal \\nabla_{a_l} C</span></p>\n<p>Transpose doesn't change diagonal matrices so</p>\n<p><span class=\"math display\">= {\\Sigma&#39; (z_l)} \\nabla_{a_l} C</span></p>"},"sha1":"6f70934e588d04e883d816fe5be976f96aba3cf9"},"kids":[]}
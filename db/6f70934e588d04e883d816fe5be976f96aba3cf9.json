{"item":{"attr":{"a":"<p>We can either prove it by using properties of diagonal matrices or\nusing the chain rule of gradient. Former one is rather mundane so I'll\ngo with latter one.</p>\n<p><span class=\"math inline\">\\delta_l</span> is gradient of <span\nclass=\"math inline\">C</span> in terms of <span\nclass=\"math inline\">z_l</span>, namely</p>\n<p><span class=\"math display\">\\delta^l = \\nabla_{z_l} C</span></p>\n<p>Since <span class=\"math inline\">a_l = \\sigma(z_l)</span>, we can\napply chain rule with Jacobian matrix.</p>\n<p><span class=\"math display\">\\nabla_{z_l} C = {J_\\sigma\n(z_l)}^\\intercal \\nabla_{a_l} C</span></p>\n<p>When the function is element-wise operation the Jacobian matrix\nbecomes a diagonal matrix. <span class=\"math inline\">\\sigma</span> is\nelementwise function therefore the Jacobian matrix becomes exactly the\nmatrix that was given in the question.</p>\n<p><span class=\"math display\">= {\\Sigma&#39; (z_l)}^\\intercal\n\\nabla_{a_l} C</span></p>\n<p>Transpose doesn't change diagonal matrices so</p>\n<p><span class=\"math display\">= {\\Sigma&#39; (z_l)} \\nabla_{a_l}\nC</span></p>","q":"<p>Show that (BP1) may be rewritten as</p>\n<p><span class=\"math display\">\\delta^L = \\Sigma&#39;(z^L) \\nabla_a C\n\\tag{33}</span></p>\n<p>where <span class=\"math inline\">\\Sigma&#39;(z^L)</span> is a square\nmatrix whose diagonal entries are the values <span\nclass=\"math inline\">\\sigma&#39;(z^L_j)</span>, and whose off-diagonal\nentries are zero. Note that this matrix acts on <span\nclass=\"math inline\">\\nabla_a C</span> by conventional matrix\nmultiplication.</p>"},"sha1":"6f70934e588d04e883d816fe5be976f96aba3cf9","title":"1"},"kids":[],"parentSha1":"6ed65dadd1a6039b895159d69c1c0d8be2959671","path":"answers-db/books/Neural Networks and Deep Learning/2. How the backpropagation algorithm works/The four fundamental equations behind backpropagation/Alternate presentation of the equations of backpropagation/1"}
{"parentSha1":"d7d70003879db33b65c3af84054af70bfcc13e39","path":"books/Neural Networks and Deep Learning/1. Using neural nets to recognize handwritten digits/Learning with gradient descent","kids":[{"attr":{"a":"By Cauchy-Schwarz inequality, $`\\nabla C \\cdot \\Delta v`$'s least possible value is $`- \\lVert \\nabla C \\rVert \\lVert \\Delta v \\rVert`$ and it only happens when $`\\Delta v`$ is colinear to $`\\nabla C`$ therefore \n\n```math\n\\text{min}(\\nabla C \\cdot \\Delta v) \\\\\n= - \\lVert \\nabla C \\rVert \\lVert \\eta \\nabla C \\rVert\n```\n\n$`\\lVert \\eta \\nabla C \\rVert`$ should be $`\\epsilon`$ therefore\n\n```math\n\\eta = {\\epsilon \\over \\lVert \\nabla C \\rVert}\n```","q":"Prove the assertion of the last paragraph. Hint: If you're not already familiar with the Cauchy-Schwarz inequality, you may find it helpful to familiarize yourself with it.\n"},"sha1":"4eecbc6e5a6ec299a9ee1a37d49dc395b3c82a6d","title":"1"},{"attr":{"a":"![](assets/IMG_0636.jpeg)","q":"I explained gradient descent when $`C`$ is a function of two variables, and when it's a function of more than two variables. What happens when $`C`$ is a function of just one variable? Can you provide a geometric interpretation of what gradient descent is doing in the one-dimensional case?"},"sha1":"5418de8899a4217c54039b7757d3ae30f8dd4b12","title":"2"}],"item":{"attr":{},"sha1":"ba5bd163172d3b720df1f50e3cb9fb437a964000","title":"Learning with gradient descent"}}
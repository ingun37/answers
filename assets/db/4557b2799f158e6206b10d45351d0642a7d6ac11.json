{"parentSha1":"b0472f3d9a6f8b52b80c5d4303f31bd8eeb95ff8","path":"books/Neural Networks and Deep Learning/2. How the backpropagation algorithm works/The four fundamental equations behind backpropagation/Alternate presentation of the equations of backpropagation/2","item":{"title":"2","attr":{"q":"Show that (BP2) may be rewritten as\n\n```math\n\\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\delta^{l+1} \\tag{34}\n```","a":"Just like the previous question, we can either prove it by using properties of diagonal matrices or applying one more chian rule to the answer of previous question. I'll go with latter one again.\n\nLet $`T`$ be the function of multiplying and adding weights and biases.\n\n```math\nT(a) = Wa + B\n```\n\nThen we can define $`z_{l+1}`$ in terms of $`z_l`$.\n\n```math\nz_{l+1} = T \\circ \\sigma (z_l)\n```\n\nJacobian of $`T \\circ \\sigma`$ is (chain rule)\n\n```math\nJ_{T \\sigma} = J_T (a_l) J_\\sigma (z_l)\n```\n\nwhere $`J_\\sigma (z_l)`$ is $`\\Sigma ' (z_l)`$ (proven in the previous problem) and $`J_T (a_l) = W`$ (because of linearity). Therefore we can simplify like this\n\n```math\n= W \\Sigma ' (z_l)\n```\n\nWe are already given the gradient of $`C`$ in terms of $`z_{l+1}`$ which is $`\\delta_{l+1}`$, therefore\n\n```math\n\\begin{aligned}\n    \\delta_l &= \\left( W \\Sigma ' (z_l) \\right)^\\intercal \\delta_{l+1} \\\\\n             &= W^\\intercal \\Sigma ' (z_l)^\\intercal \\delta_{l+1} \\\\\n\\end{aligned}\n```\n\nSince $`\\Sigma ' (z_l)`$ is diagonal we can remove the transpose and move it to the end and turn into Hadamard product.\n\n```math\n= W^\\intercal \\delta_{l+1} \\odot \\sigma ' (z_l)\n```"},"sha1":"4557b2799f158e6206b10d45351d0642a7d6ac11"},"kids":[]}
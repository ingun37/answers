{"parentSha1":"4807c04a2a44d49ad22ff425c5b889cdd6d5dff2","path":"books/Neural Networks and Deep Learning/2. How the backpropagation algorithm works/The four fundamental equations behind backpropagation/Alternate presentation of the equations of backpropagation","item":{"title":"Alternate presentation of the equations of backpropagation","attr":{"q":"I've stated the equations of backpropagation (notably (BP1) and (BP2)) using the Hadamard product. This presentation may be disconcerting if you're unused to the Hadamard product. There's an alternative approach, based on conventional matrix multiplication, which some readers may find enlightening."},"sha1":"b0472f3d9a6f8b52b80c5d4303f31bd8eeb95ff8"},"kids":[{"title":"1","attr":{"q":"Show that (BP1) may be rewritten as\n\n```math\n\\delta^L = \\Sigma'(z^L) \\nabla_a C\n\\tag{33}\n```\n\nwhere $`\\Sigma'(z^L)`$ is a square matrix whose diagonal entries are the values $`\\sigma'(z^L_j)`$, and whose off-diagonal entries are zero. Note that this matrix acts on $`\\nabla_a C`$ by conventional matrix multiplication.","a":"We can either prove it by using properties of diagonal matrices or using the chain rule of gradient. Former one is rather mundane so I'll go with latter one.\n\n$`\\delta_l`$ is gradient of $`C`$ in terms of $`z_l`$, namely\n\n```math\n\\delta^l = \\nabla_{z_l} C\n```\n\nSince $`a_l = \\sigma(z_l)`$, we can apply chain rule with Jacobian matrix.\n\n```math\n\\nabla_{z_l} C = {J_\\sigma (z_l)}^\\intercal \\nabla_{a_l} C \n```\n\nWhen the function is element-wise operation the Jacobian matrix becomes a diagonal matrix. $`\\sigma`$ is elementwise function therefore the Jacobian matrix becomes exactly the matrix that was given in the question.\n\n```math\n= {\\Sigma' (z_l)}^\\intercal \\nabla_{a_l} C \n```\n\nTranspose doesn't change diagonal matrices so\n\n```math\n= {\\Sigma' (z_l)} \\nabla_{a_l} C \n```"},"sha1":"cb91e3c897f441cd40064631250662d0ff548040"},{"title":"3","attr":{"q":"By combining observations (1) and (2) show that\n\n```math\n\n    \\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\ldots \\Sigma'(z^{L-1}) (w^L)^T \n    \\Sigma'(z^L) \\nabla_a C\n  \\tag{35}\n```","a":"We've already proved that Hadamard productinng $`\\sigma ' (z_l)`$ is equal to multiplying $`\\Sigma ' (z_l)`$, and $`\\Sigma ' (z_l)`$ can move it's multiplicative position. Rearranging the result of (2) we get\n\n```math\n\\delta_l = W^\\intercal \\delta_{l+1} \\odot \\sigma ' (z_l) = W^\\intercal \\Sigma ' (z_l) \\delta_{l+1}\n```\n\nwhich is recursive expression with the initial value of $`\\delta^L = \\Sigma'(z^L) \\nabla_a C`$. We get the expression from the question when we serialize this recursive expression."},"sha1":"bca9b4ed2ed787138c17997ef402fd20d8c99985"},{"title":"2","attr":{"q":"Show that (BP2) may be rewritten as\n\n```math\n\\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\delta^{l+1} \\tag{34}\n```","a":"Just like the previous question, we can either prove it by using properties of diagonal matrices or applying one more chian rule to the answer of previous question. I'll go with latter one again.\n\nLet $`T`$ be the function of multiplying and adding weights and biases.\n\n```math\nT(a) = Wa + B\n```\n\nThen we can define $`z_{l+1}`$ in terms of $`z_l`$.\n\n```math\nz_{l+1} = T \\circ \\sigma (z_l)\n```\n\nJacobian of $`T \\circ \\sigma`$ is (chain rule)\n\n```math\nJ_{T \\sigma} = J_T (a_l) J_\\sigma (z_l)\n```\n\nwhere $`J_\\sigma (z_l)`$ is $`\\Sigma ' (z_l)`$ (proven in the previous problem) and $`J_T (a_l) = W`$ (because of linearity). Therefore we can simplify like this\n\n```math\n= W \\Sigma ' (z_l)\n```\n\nWe are already given the gradient of $`C`$ in terms of $`z_{l+1}`$ which is $`\\delta_{l+1}`$, therefore\n\n```math\n\\begin{aligned}\n    \\delta_l &= \\left( W \\Sigma ' (z_l) \\right)^\\intercal \\delta_{l+1} \\\\\n             &= W^\\intercal \\Sigma ' (z_l)^\\intercal \\delta_{l+1} \\\\\n\\end{aligned}\n```\n\nSince $`\\Sigma ' (z_l)`$ is diagonal we can remove the transpose and move it to the end and turn into Hadamard product.\n\n```math\n= W^\\intercal \\delta_{l+1} \\odot \\sigma ' (z_l)\n```"},"sha1":"4557b2799f158e6206b10d45351d0642a7d6ac11"}]}
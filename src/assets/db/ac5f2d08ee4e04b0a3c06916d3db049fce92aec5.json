{"parentSha1":"ba5bd163172d3b720df1f50e3cb9fb437a964000","path":"books/Neural Networks and Deep Learning/1. Using neural nets to recognize handwritten digits/Learning with gradient descent/3","item":{"title":"3","attr":{"q":"An extreme version of gradient descent is to use a mini-batch size of just 1. That is, given a training input, $`x`$, we update our weights and biases according to the rules $`w_k \\rightarrow w_k' = w_k - \\eta \\partial C_x / \\partial w_k`$ and $`b_l \\rightarrow b_l' =  b_l - \\eta \\partial C_x / \\partial b_l`$. Then we choose another training input, and update the weights and biases again. And so on, repeatedly. This procedure is known as online, on-line, or incremental learning. In online learning, a neural network learns from just one training input at a time (just as human beings do). Name one advantage and one disadvantage of online learning, compared to stochastic gradient descent with a mini-batch size of, say, 20.","a":"### Advantages\n\nComputation speed.\n\n### Disadvantages\n\nPoorly estimatied $`C`$."},"sha1":"ac5f2d08ee4e04b0a3c06916d3db049fce92aec5"},"kids":[]}
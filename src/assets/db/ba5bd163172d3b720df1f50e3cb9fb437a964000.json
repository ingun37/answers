{"parentSha1":"d7d70003879db33b65c3af84054af70bfcc13e39","path":"books/Neural Networks and Deep Learning/1. Using neural nets to recognize handwritten digits/Learning with gradient descent","kids":[{"attr":{"a":"By Cauchy-Schwarz inequality, $`\\nabla C \\cdot \\Delta v`$'s least possible value is $`- \\lVert \\nabla C \\rVert \\lVert \\Delta v \\rVert`$ and it only happens when $`\\Delta v`$ is colinear to $`\\nabla C`$ therefore \n\n```math\n\\text{min}(\\nabla C \\cdot \\Delta v) \\\\\n= - \\lVert \\nabla C \\rVert \\lVert \\eta \\nabla C \\rVert\n```\n\n$`\\lVert \\eta \\nabla C \\rVert`$ should be $`\\epsilon`$ therefore\n\n```math\n\\eta = {\\epsilon \\over \\lVert \\nabla C \\rVert}\n```","q":"Prove the assertion of the last paragraph. Hint: If you're not already familiar with the Cauchy-Schwarz inequality, you may find it helpful to familiarize yourself with it.\n"},"sha1":"4eecbc6e5a6ec299a9ee1a37d49dc395b3c82a6d","title":"1"},{"attr":{"a":"### Advantages\n\nComputation speed.\n\n### Disadvantages\n\nPoorly estimatied $`C`$.","q":"An extreme version of gradient descent is to use a mini-batch size of just 1. That is, given a training input, $`x`$, we update our weights and biases according to the rules $`w_k \\rightarrow w_k' = w_k - \\eta \\partial C_x / \\partial w_k`$ and $`b_l \\rightarrow b_l' =  b_l - \\eta \\partial C_x / \\partial b_l`$. Then we choose another training input, and update the weights and biases again. And so on, repeatedly. This procedure is known as online, on-line, or incremental learning. In online learning, a neural network learns from just one training input at a time (just as human beings do). Name one advantage and one disadvantage of online learning, compared to stochastic gradient descent with a mini-batch size of, say, 20."},"sha1":"ac5f2d08ee4e04b0a3c06916d3db049fce92aec5","title":"3"},{"attr":{"a":"![](assets/IMG_0636.jpeg)","q":"I explained gradient descent when $`C`$ is a function of two variables, and when it's a function of more than two variables. What happens when $`C`$ is a function of just one variable? Can you provide a geometric interpretation of what gradient descent is doing in the one-dimensional case?"},"sha1":"5418de8899a4217c54039b7757d3ae30f8dd4b12","title":"2"}],"item":{"attr":{},"sha1":"ba5bd163172d3b720df1f50e3cb9fb437a964000","title":"Learning with gradient descent"}}